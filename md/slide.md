# テンソル分解

---

## テンソルとは？

機械学習の分野では単なる多次元配列でいいよ

２次元配列の拡張だよ

---

## 協調フィルタリング

この枠組みで行列分解、ひいてはテンソル分解を議論するよ

---

## 協調フィルタリングとは？


> "協調フィルタリング（きょうちょうフィルタリング、Collaborative Filtering、CF）は、
多くのユーザの嗜好情報を蓄積し、
あるユーザと嗜好の類似した他のユーザの情報を用いて
自動的に推論を行う方法論である。"(wikipediaより)

--

そうなんや。

--

### 結局？

Amazonのおすすめ商品とか

--

### 手法は？

+ メモリベース
    - ユーザー間の類似度を計算してうんぬんかんぬん（ググったらよくでてくるのはこっち）
+ モデルベース
    - 今からやるよ

---

## モデル（２次元）

+ $R\_{n,m}$:ユーザーnのアイテムmに対する評価値
+ $U\_{d,n}$:ユーザーnの特徴dに対する評価値
+ $V\_{d,m}$:ユーザーnの特徴dに対する評価値
+ $\epsilon\_{n,m}$:ノイズ項

$$
R\_{n,m}=\sum\_{d=1}^DU\_{d,n}V\_{d,m}+\epsilon\_{n,m}
$$

行列っぽく書き換えて

$$
\mathbf{R}\simeq\mathbf{U}^\top\mathbf{V}
$$

--

$\bf{R}$を観測して、$\bf{U},\bf{V}$を推定したい。

+ ユーザーの嗜好を分析できる。
+ アイテムの特徴を分析できる。

これらは本来潜在的な値。

ここから、未観測レーティングを予測補完できる。


--

$$
\mathbf{R}\simeq\mathbf{U}^\top\mathbf{V}
$$

ものすごく線形次元削減or非負値行列因子分解


---

## テンソル分解

今までの議論にもう一個次元を足す。

例えば、流行の変化とか。

$$
R\_{n,m,k}=\sum\_{d=1}^DU\_{d,n}V\_{d,m}S\_{d,k}+\epsilon\_{n,m,k}
$$

+ $S\_{d,k}$:時刻$k$における特徴dの流行度

--

やりたいことは変わらない。次元が一個増えただけ。

$\bf{R}$を観測して、$\bf{U},\bf{V}, \bf{S}$を推定したい。


---


ここからは数式をこねくり回す話

--

### モデル

--

$$
R\_{n,m,k}=\sum\_{d=1}^DU\_{d,n}V\_{d,m}S\_{d,k}+\epsilon\_{n,m,k}
$$

ノイズ項$\epsilon\_{n,m,k}$が$N(0,\lambda^{-1})$に従うとする

--

ユーザー及びアイテムの特徴ベクトル$\bf{U}\_{:,n},\bf{V}\_{:,m}$は

多次元ガウス分布に従うとする

--

流行を表す$\bf{S}$については、

時系列を考慮して、

マルコフ性を仮定したガウス分布を考える。

--

以上の確率分布にはもちろんパラメータ（母数）がある

これも学習したい

ガンマ分布とかガウス・ウィシャート分布とか…

--

そのへんは前の方のページ見て

--

というわけで同時分布は

$$
\begin{align}
&p(\mathbf{R},\mathbf{U},\mathbf{V},\mathbf{S},\lambda,
\mathbf{\mu}\_U,\mathbf{\mu}\_V,\mathbf{\mu}\_S,
\mathbf{\Lambda}\_U,\mathbf{\Lambda}\_V,\mathbf{\Lambda}\_S)\\\\
=&p(\mathbf{R}\|\mathbf{U},\mathbf{V},\mathbf{S},\lambda)
p(\mathbf{U}\|\mathbf{\mu}\_{U},\mathbf{\Lambda}\_{U})
p(\mathbf{V}\|\mathbf{\mu}\_{V},\mathbf{\Lambda}\_{V})
p(\mathbf{U}\|\mathbf{\mu}\_{S},\mathbf{\Lambda}\_{S})\\\\
&\times p(\lambda)
p(\mu\_U,\Lambda\_U)
p(\mu\_V,\Lambda\_V)
p(\mu\_S,\Lambda\_S)
\end{align}
$$

--

### グラフィカルモデル

<img src="./img/IMG_1725.jpg" height=500>

---

### 変分推論

n回目の変分推論

時間方向は、完全分解変分推論と同様に。

--

パラメータを$\Theta$にまとめると、事後分布は

$$
p(\bf{U},\bf{V},\bf{S},\bf{\Theta}\|\bf{R})=\frac{p(\bf{R},\bf{U},\bf{V},\bf{S},\bf{\Theta})}{p(\bf{R})}
$$

--

$p(\bf{R})$は厳密には計算できない(n回目)

--

なので、近似

--

$$
\begin{align}
p(\bf{U},\bf{V},\bf{S},\bf{\Theta}\|\bf{R})&\approx q(\bf{U})q(\bf{V})q(\bf{S})q(\bf{\Theta})\\\\
&=q(\bf{U})q(\bf{V})\left\\\{\prod\_{k=1}^K q(\bf{S}\_{:,k})\right\\\}q(\bf{\Theta})
\end{align}
$$

--

あとはいつもどおり

変分推論の公式とかいうやつで近似分布を出す。

--

本見て

--

数式が大変そうだなあ

--

質問があれば聞きますが（答えられるとは言ってない）、

特になければ面倒なので飛ばします

--

計算を頑張ると、更新式が得られる。

---

# By the way

--

潜在変数の時系列を

線形なガウス分布の連鎖で表したものを

**線形動的システム**という。

隠れマルコフモデルの連続値版とみなせる。

これらの時系列モデルは総称して**状態空間モデル**と呼ばれる。

--

今回のような$q(\bf{S})$の分解を仮定しなくても、

周辺分布を求めれられる。

結果として得られるのは

**カルマンフィルタ**。

また、モデルの線形性を仮定せずに

ヤコビアンとかでなんとかしたのを

**拡張カルマンフィルタ**という。

---

## 欠損値の補完

実際は$\bf{R}$はほとんどの値が欠損値となる

線形次元削減による欠損値補完のアイデアが

そのまま使えて

$$
\ln q(R\_{n,m,k}) = \left<\ln N\left(R\_{n,m,k}\|\sum\_{d=1}^DU\_{d,n}V\_{d,n}S\_{d,k}\right)\right>\_{q(\bf{U},\bf{V},\bf{S},\lambda)}
$$

--

他の変数の近似分布についても

$R\_{n,m,k}$が欠損値となっているところは

$$
\left< R\_{n,m,k}\right>=\sum\_{d=1}^DU\_{d,n}V\_{d,n}S\_{d,k}
$$
$$
\left< R^2\_{n,m,k}\right>=\left< R\_{n,m,k}\right>^2-\left<\lambda\right>^{-1}
$$

とすればよい
